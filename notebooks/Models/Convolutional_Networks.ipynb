{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7234ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import time\n",
    "import shutil\n",
    "import collections\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cfb5f1-79ed-40bd-8db1-27180c6b16b5",
   "metadata": {},
   "source": [
    "### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440803b0-2f0b-45a9-96ab-68da77797eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "here_path = Path().resolve()\n",
    "repo_path = here_path.parents[1]\n",
    "sys.path.append(str(repo_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce7334-8287-4b8a-b6a0-3d952d91d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py.utils import verifyDir, verifyFile, verifyType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afacf91-ea4e-4c10-af2e-ecd6d8576bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py.config import Config\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "np.random.seed(cfg.RANDOM_STATE)\n",
    "cfg.DATA_PATH, cfg.MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76f508-8c42-4e66-bcc5-8205d149a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "QSCORE_PATH=f\"{cfg.DATA_PATH}pp1/Qscores/\"\n",
    "IMAGES_PATH = f\"{cfg.DATA_PATH}pp1/images/\"\n",
    "MODEL_PATH = f\"{cfg.MODEL_PATH}pp1/{cfg.YEAR_STUDIED}/cnn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbeedf-241e-4cda-b335-c31190ad9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "verifyDir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db9ce3-42b7-4957-b395-c31c4540feb4",
   "metadata": {},
   "source": [
    "### Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cd3c8-ec42-4f77-8dff-b7c55db6b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_type = torch.float32 if device.type == \"cuda\" else torch.float16\n",
    "device, torch_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74de26-e62d-472c-8cfe-04341f147d07",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e45bd-3bcb-4540-b652-6e2c47a6552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 1 if \"reg\" in cfg.ML_TASK else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd546c-52f8-4169-ad87-5b54b88d49c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data_df = pd.read_csv(f\"{QSCORE_PATH}scores.csv\", sep=\";\", low_memory=False)\n",
    "data_df[\"image_path\"] = f\"{IMAGES_PATH}{cfg.YEAR_STUDIED}/\" + data_df[\"image_path\"]\n",
    "data_df[\"image_id\"] = data_df[\"image_id\"].apply(str)\n",
    "data_df.sort_values(by=[cfg.PERCEPTION_METRIC], ascending=False, inplace=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e3538-529b-4c9c-a42f-cd47bbed112c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from py.models.datasets.transformations import ImageTransforms\n",
    "\n",
    "transforms_list = ImageTransforms().get(model_name=cfg.MODEL_FEATURE_NAME)\n",
    "transforms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a42e60-cbbd-4635-87ca-65bb6515476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class ImagesLabels(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.image_paths = dataset[\"image_path\"].tolist()\n",
    "        self.targets = dataset[\"target\"].tolist()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A single sample (image, label) where the label can be inferred from the filename or other metadata.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        \n",
    "        # Apply any transforms if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Example label from filename (e.g., assuming format class_index.jpg)\n",
    "        label = self.targets[idx]\n",
    "\n",
    "        return {\"images\": image, \"targets\": label }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64a497-8b4e-434d-bbe4-5ab28f44d093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from py.models.datasets import PlacePulse\n",
    "\n",
    "pp = PlacePulse(data_df)\n",
    "pp.DataPreparation(delta=cfg.DELTA, emotion=cfg.PERCEPTION_METRIC)\n",
    "pp.TaskPreparation(task_type=cfg.ML_TASK)\n",
    "pp.DataSplit()\n",
    "pp.DataFormat(data_formater=ImagesLabels, transforms_list=transforms_list)\n",
    "pp.DataLoader(batch_size=cfg.BATCH_SIZE, shuffle_train=False)\n",
    "pp.plot()\n",
    "\n",
    "print(f\"Train samples: {len(pp.train_df)}\")\n",
    "print(f\"Test samples: {len(pp.test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff9ccb",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23767e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from py.models.classification.cnn import ConvolutionClassifier\n",
    "\n",
    "tm = ConvolutionClassifier(model_name=cfg.MODEL_FEATURE_NAME, num_classes=NUM_CLASSES)\n",
    "\n",
    "tm.to_device(device)\n",
    "tm.model_zoo()\n",
    "tm.print_trainable_parameters(log_params=True)\n",
    "tm.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53236fe6",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = pp.dataloaders[\"train\"]\n",
    "val_loader = pp.dataloaders[\"val\"]\n",
    "\n",
    "criterion = nn.MSELoss() if \"reg\" in cfg.ML_TASK else nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "                filter(lambda p: p.requires_grad, tm.model.parameters()), \n",
    "                lr=1e-4,\n",
    "                weight_decay=5e-4,\n",
    "            )\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                    optimizer, T_max=cfg.NUM_EPOCHS, eta_min=1e-6\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122229ea-61ae-4d28-a67f-b1cc77edc04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py.models.metrics import EvaluationMetrics\n",
    "\n",
    "task_metrics = EvaluationMetrics(task=cfg.ML_TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b66bb2-fc39-4b7b-bdce-e16b0326722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_step(data_loader, \n",
    "                        model, \n",
    "                        optimizer,\n",
    "                        criterion,\n",
    "                        task_metrics,\n",
    "                        cur_epoch,\n",
    "                        is_train=True):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pred_targets = []\n",
    "    real_targets = []\n",
    "\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for i, (batch) in enumerate(data_loader):\n",
    "            batch_images = batch['images'].to(device)\n",
    "            batch_targets = batch['targets'].to(device)\n",
    "\n",
    "            # Forward\n",
    "            batch_predictions = model(batch_images)\n",
    "            loss = criterion(batch_predictions, batch_targets)\n",
    "            \n",
    "            # Backward\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "            # Store predictions and labels\n",
    "            if \"reg\" in cfg.ML_TASK:\n",
    "                pred_targets.extend(batch_predictions.cpu().detach().numpy())\n",
    "            else:\n",
    "                _, predicted = torch.max(batch_predictions, 1)\n",
    "                pred_targets.extend(predicted.cpu().detach().numpy())\n",
    "            \n",
    "            real_targets.extend(batch_targets.cpu().detach().numpy())\n",
    "            \n",
    "            if i % 100 == 0 and is_train:\n",
    "                print(f\"Epoch [{cur_epoch+1}/{cfg.NUM_EPOCHS}] | Step [{i}/{len(data_loader)}] | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader)  # was len(train_loader), should match the loader passed in\n",
    "    if is_train:\n",
    "        print(f'Train')\n",
    "    else:\n",
    "        print(f'Validation')\n",
    "    print(f'Loss: {epoch_loss:.4f}')\n",
    "    eval_metrics = task_metrics.calculate(real_targets, pred_targets)\n",
    "    return eval_metrics, epoch_loss, real_targets, pred_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a25cf-fa6b-4bb2-b1d6-bf4ac776064d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best_val_metric = float(\"inf\") if \"reg\" in cfg.ML_TASK else 0.0 \n",
    "\n",
    "for epoch in range(cfg.NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{cfg.NUM_EPOCHS}:\")\n",
    "\n",
    "    # Training Phase\n",
    "    start = time.time()\n",
    "    train_metrics, train_loss, train_targets, train_predictions = classification_step(\n",
    "                                                                    data_loader=train_loader,\n",
    "                                                                    model=tm.model,\n",
    "                                                                    optimizer=optimizer,\n",
    "                                                                    criterion=criterion,\n",
    "                                                                    task_metrics=task_metrics,\n",
    "                                                                    cur_epoch=epoch,\n",
    "                                                                    is_train=True\n",
    "                                                                )\n",
    "    end = time.time()\n",
    "    print(f\"Training time elapsed: {(end - start)/60:.4f} minutes\\n\")\n",
    "\n",
    "    # Validation Phase\n",
    "    start = time.time()\n",
    "    val_metrics, val_loss, val_real_targets, val_predictions = classification_step(\n",
    "                                                                    data_loader=val_loader,\n",
    "                                                                    model=tm.model,\n",
    "                                                                    optimizer=optimizer,\n",
    "                                                                    criterion=criterion,\n",
    "                                                                    task_metrics=task_metrics,\n",
    "                                                                    cur_epoch=epoch,\n",
    "                                                                    is_train=False\n",
    "                                                                )\n",
    "    end = time.time()\n",
    "    print(f\"Validation time elapsed: {(end - start)/60:.4f} minutes\\n\")\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save the best model if validation accuracy improves\n",
    "    current_metric = val_metrics[\"mse\"] if \"reg\" in cfg.ML_TASK else val_metrics[\"accuracy\"]\n",
    "    is_better = (\"reg\" in cfg.ML_TASK and current_metric < best_val_metric) or \\\n",
    "                (\"class\" in cfg.ML_TASK and current_metric > best_val_metric)\n",
    "        \n",
    "    if is_better:\n",
    "        best_val_metric = current_metric\n",
    "        torch.save(tm.model.state_dict(), f\"{MODEL_PATH}{cfg.MODEL_FEATURE_NAME}_best_model.pth\")\n",
    "        print(f\"âœ… Epoch {epoch+1}: New best model saved! Metric: {best_val_metric:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
